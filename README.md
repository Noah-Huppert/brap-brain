# Brap Brain
Run GTP-2 trained on Discord messages.

# Table Of Contents
- [Overview](#overview)
- [Instructions](#instructions)

# Overview
The goal of this project is to try and train GPT-2 on your Discord server's message history. It includes OpenAI's GPT-2 model code and nshepperd's training code. As well as the glue code to generate training datasets from Discord messages and run evaluations on the model.

Shoutout to nshepperd and Ng Wai Foong. [Ng Wai Foong's blog post](https://medium.com/ai-innovation/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f) and [nshepperd's GitHub GTP-2 fork](https://github.com/nshepperd/gpt-2) provided some very helpful code and instructions. The `encode.py`, `load_datasets.py`, and `train.py` files in the `src/` directory are from nshepperd's fork.

# Instructions
## Train the model
Docker and Docker Compose must be installed.

1. Copy `.env-example` and name it `.env`, fill in your own values
  - The `DISCORD_TOKEN` value must be for a bot which has the `bot` scope with the Bot Permissions `Read Messages/View Channels` and `Read Message History`. Invite this bot using the URL generated by the Discord Developer Dashboard OAuth2 > URL Generator page
2. Run a script to download all messages from the Discord server:
   ```bash
   ./scripts/download-discord.sh
   ```
3. Start the Docker Compose stack:
   ```bash
   docker-compose up -d --build
   ```
4. Then run a script to transform Discord message files into training data:
   ```bash
   ./scripts/make-training-data.sh
   ```
5. Train the model:
   ```bash
   docker-compose run --rm model python3 src/train.py --dataset ./training-data/discord-messages.npz
   ```
   Every 1,000 steps a snapshot of the training will be saved, after this happens you can safely CTRL+C and training results will be saved.  
   The `--run_name <checkpoint run name>` option can be provided to customize the sub-directory in `checkpoints/` in which results will be saved.  
   The `--sample_every <num>` option can be provided to change how often samples are taken from the model.  
   The `--save_every <num>` option can be provided to change how often checkpoints are saved.
   The `--tf_dev <device>` option can be used to train the model on a specific device (such as a GPU). To see avaliable devices provide the `--list_tf_devs` option. See [GPU Support](#gpu-support) for instructions on how to install the required libraries.  
   The `--tpu_addr <node name>` option can be used to enable experimental TPU support.
6. Use the trained data:
   - The `checkpoints/<run name>/` directory will have a bunch of files which need to be moved into the `models/` directory in order for the GPT-2 code to use the training results. Run the following to copy the data:
     ```bash
     ./scripts/copy-checkpoint-to-model.sh -m <model name> -r <checkpoint run name> -i <iteration count>
     ```
     Make sure to fill in the placeholders:
     - `<model name>` is the name of the model you want to create from the checkpoint files, this will determine the name of the sub-directory inside the `models/` directory
     - `<checkpoint run name>` is the name of the sub-directory inside `checkpoints/` from which files will be copied, by default this is `run1` and can be customized during the training step via the `--run_name` option
     - `<iteration count` is the training iteration number from which model weights will be used, if you look in `checkpoints/<checkpoint run name>/` you will see a bunch of files post-fixed with numbers, each of these numbers if the `<iteration count>`, ideally you should pick the highest number you see in this directory
   - Then in `src/interactive_conditional_samples.py` or `src/generate_unconditional_samples.py` change the `model_name` argument of the main function to `<model name>`

## GPU Support
Running training on a GPU can greatly increase the speed compared to running on a CPU. However, a few things must be setup first, and it can be a little tricky.

The main rule of thumb is to pay attention to error messages at the beginning of the program's output, and try to resolve those. Once all error messages are gone you will be left with an environment where GPU training will be possible. Instructions assume you are using Nvidia graphics cards which support CUDA.

To setup GPU training:

1. Install [Nvida CUDA v10](https://developer.nvidia.com/cuda-10.0-download-archive).  
   Installing version 10 (not the latest) is important. As the GPT-2 code uses Tensorflow 1.15.5, and as you can see from this [Tensorflow compatibility table](https://www.tensorflow.org/install/source#gpu) CUDA v10 is the only compatible version.  
   You will be required to create an Nvidia developer account before you can download CUDA.
2. Install [cuDNN v7.4](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse742-10).  
   Again the specific version is important here. The compatibility table from the previous step indicates cuDNN v7.4 is the only compatible version.  
   On Windows the installation process is a little confusing. Once you download cuDNN extract the ZIP file and open the `cuda` folder. Inside this folder there should be a `lib`, `include`, and `bin` folder. Find the location where CUDA is installed (Likely `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0`). Then copy the `lib`, `include`, and `bin` folders from cuDNN into the CUDA directory. If prompted select to overwrite any files.
3. Install [Conda](https://docs.conda.io/en/latest/).  
   Although it is not my favorite Python virtual environment or package manager it is able to install some required components.
4. Install Python libraries for CUDA and cuDNN by running:
   ```
   conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
   ```
5. To simplify the situation it is recommended you perform GPU training outside of Docker. Since this is the case you should create a virtual environment by running:
   ```
   conda create -n brap-brain python=3.7
   conda activate brap-brain
   ```
   Run future commands in this same terminal session. If you make a new session be sure to run `conda activate brap-brain` before completing the following steps.
6. Install Python dependencies into the Conda virtual environment by running:
   ```
   pip3 install -r ./requirements.txt
   ```
7. At this point the environment should be ready to train using a GPU. To verify this ask the training script to list the devices available to it:
   ```
   python3 ./src/train.py --list_tf_devs
   ```
   If you see something like `/device:GPU:0` in the list then your GPU is ready to be used. If you do not see your GPU in the list then check the log output to see if there are any errors which you can work through.  
   To train with your GPU simply provide the `--tf_dev <device>` option to the script (ex., `--tf_dev GPU:0`). This option lets you be lienient with the name of the device, and you do not have to type the `/device:` part as seen in the `--list_tf_devs` output.