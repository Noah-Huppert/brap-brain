# Brap Brain
Run GTP-2 trained on Discord messages.

# Table Of Contents
- [Overview](#overview)
- [Instructions](#instructions)

# Overview
The goal of this project is to try and train GPT-2 on your Discord server's message history. It includes OpenAI's GPT-2 model code and nshepperd's training code. As well as the glue code to generate training datasets from Discord messages and run evaluations on the model.

Shoutout to nshepperd and Ng Wai Foong. [Ng Wai Foong's blog post](https://medium.com/ai-innovation/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f) and [nshepperd's GitHub GTP-2 fork](https://github.com/nshepperd/gpt-2) provided some very helpful code and instructions. The `encode.py`, `load_datasets.py`, and `train.py` files in the `src/` directory are from nshepperd's fork.

# Instructions
## Train the model
Docker and Docker Compose must be installed.

1. Copy `.env-example` and name it `.env`, fill in your own values
  - The `DISCORD_TOKEN` value must be for a bot which has the `bot` scope with the Bot Permissions `Read Messages/View Channels` and `Read Message History`. Invite this bot using the URL generated by the Discord Developer Dashboard OAuth2 > URL Generator page
2. Run a script to download all messages from the Discord server:
   ```bash
   ./scripts/download-discord.sh
   ```
3. Start the Docker Compose stack:
   ```bash
   docker-compose up -d --build
   ```
4. Then run a script to transform Discord message files into training data:
   ```bash
   ./scripts/make-training-data.sh
   ```
5. Train the model:
   ```bash
   docker-compose run --rm model python3 src/train.py --dataset ./training-data/discord-messages.npz
   ```
   Every 1,000 steps a snapshot of the training will be saved, after this happens you can safely CTRL+C and training results will be saved.  
   The `--run_name <checkpoint run name>` option can be provided to customize the sub-directory in `checkpoints/` in which results will be saved.  
   The `--sample_every <num>` option can be provided to change how often samples are taken from the model.  
   The `--save_every <num>` option can be provided to change how often checkpoints are saved.
6. Use the trained data:
   - The `checkpoints/<run name>/` directory will have a bunch of files which need to be moved into the `models/` directory in order for the GPT-2 code to use the training results. Run the following to copy the data:
     ```bash
     ./scripts/copy-checkpoint-to-model.sh -m <model name> -r <checkpoint run name> -i <iteration count>
     ```
     Make sure to fill in the placeholders:
     - `<model name>` is the name of the model you want to create from the checkpoint files, this will determine the name of the sub-directory inside the `models/` directory
     - `<checkpoint run name>` is the name of the sub-directory inside `checkpoints/` from which files will be copied, by default this is `run1` and can be customized during the training step via the `--run_name` option
     - `<iteration count` is the training iteration number from which model weights will be used, if you look in `checkpoints/<checkpoint run name>/` you will see a bunch of files post-fixed with numbers, each of these numbers if the `<iteration count>`, ideally you should pick the highest number you see in this directory
   - Then in `src/interactive_conditional_samples.py` or `src/generate_unconditional_samples.py` change the `model_name` argument of the main function to `<model name>`
